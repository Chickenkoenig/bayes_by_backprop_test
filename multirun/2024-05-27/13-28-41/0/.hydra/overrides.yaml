- model.architecture.activation=ReLU
- train.epochs=8000
- train.learning_rate=0.01
- +experiment=large_sin_gap/mlp_ensemble_exp
